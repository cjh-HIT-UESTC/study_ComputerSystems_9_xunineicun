进程是与其他进程共享CPU的    所以随着对CPU的需求增长 必然会导致内存不够用 或者 进程A本来在使用内存A 结果进程B也使用了内存A导致了错误  于是产生了虚拟内存
虚拟内存：是对主存的一种抽象概念 它为每个进程提供了一个 1、足够大的 2、一致的 3、私有的 地址空间
最核心的思想是 *****************************把主存看作 存储在磁盘上的  地址空间的高速缓存 ****************因为看作了缓存 所以能极大提高速度（相当于用了磁盘的一部分来缓存了）
9.1
物理寻址：主存 被组织成 由M个连续的字节的单元（很多） 组成的数组   每个单元对应一个唯一的物理地址    CUP访问就是直接通过物理地址直接访问主存
现在：虚拟寻址：
CPU 先 生成一个虚拟地址（VA）   （利用的是存放在主存中的查询表 所以实际在磁盘上）
然后 这个虚拟地址 被 转换为一个物理地址  这个翻译的东西叫做 内存管理单元（MMU）
9.2
虚拟地址空间：由计算机32位或者64决定的大小，每一个数代表一个虚拟地址
物理地址空间：  每一个数代表 物理内存的每一个  ***字节***（按字节来排列的数组） 
      所以 很简单的可以知道  虚拟地址空间 大于 物理地址空间  （所以直接说明了利用虚拟地址空间能扩大使用的地址空间，并增强了操作性）
 ******核心*****
 所以 地址只是指向每个数据对象（就像一个地方可以有多种描述方式） 所以 每个数据对象可以有多个地址 这些地址来自不同的地址空间
所以：主存中 每个字节 都有一个来自虚拟地址空间的虚拟地址  和  来自物理地址空间的物理地址  （但都是指向的这个数据对象）
9.3
虚拟内存是 被组织为 一个存放在###磁盘上###的 N个连续的字节单元 组成的 数组      （所以虚拟内存是在磁盘上的）
然后 再把虚拟内存（这个数组）进行分割 分割成大小固定的块 称为 虚拟页（VP）假设每页为P个字节  （现在就是以字节为单位了）
同样的 把物理内存也等分割为 物理页（PP）也叫页帧 大小也为P（与VP相同）

所以现在 虚拟页面中 就有三种：
1、比如有1000个字节单元  只拿了前500个来分割 所以还有500字节没有分割  所以这些叫做未分配的 没有任何数据和它们相关联 所以不占用任何磁盘空间
2、缓存的 在被分割的里面 缓存在物理内存的那些   （缓存在DRAM里 所以比在磁盘里取要更快）
3、未缓存的 同上但是没有缓存
9.3.1
DRAM 用来缓存虚拟页
因为如果DRAM不命中的话 消耗很大 所以把虚拟页做得很大 这样能增加命中率
9.3.2
页表 是 一个存放在物理内存的*****数据结构******   是一种数组
页表 将虚拟页映射到物理页(DRAM),将虚拟地址转换为物理地址
页表 是一种数组 每个虚拟页在页表中 一个固定偏移量处（相当于有标识位 和其他的数据 这里仅指标识位）  都有一个PTE
PTE 由有效位和n位地址字段组成 根据标识位 就可以判断是否缓存或者是否未被分配（比如设置为1和0，再加上未设置，就有三种状态）
9.3.3
页命中：既是如何根据页表取找物理地址
过程：先来一个虚拟地址 利用虚拟地址作为索引 去页表中找对应的PTE位 根据PTE的信息可以判断想要的数据对象 是否缓存在内存中了：如果已经缓存，就用PTE的物理地址去读取数据。如果没有缓存，再利用算法来替换牺牲页
9.3.4
缺页：即是DRAM缓存不命中    就是说：在页表里发现（利用PTE）对应的数据对象没有被缓存，那么会使用牺牲页算法（按需页面调度算法），然后再把要取的数据对象从磁盘复制到牺牲页对应的内存中，再取（相当于先缓存，再取数据）
9.3.5
如代码 malloc或者new  实际是在页表中 直接创建一个新的页面（新的PTE） 并在磁盘上新创建一个空间（虚拟内存）  使得这个PTE指向这个空间   核心：***在已有的基础上 直接添加创建新的页与空间
（我猜 正是因为这种直接添加的性质 所以才需要在创建一个新的空间后释放它   delet）
9.3.6
尽管上述步骤当中 看起来需要页表去进行页面调度  但由于有局部性 所以性能很好
这是因为 程序在运行之后 会趋向于在一个较小的活动页面集合上工作（工作集）：在初始开销中，将工作集页面调度到内存中，于是就很好的利用了局部性，使得命中率很高
相当于   程序的运行地址会趋向于*******集中********
9.4
上述的东西可以总结为：利用DRAM缓存比它更大的虚拟地址空间的页面，使得速度更快
需要注意的是：操作系统为每个进程提供了一个独立的页表（相当于 每个进程有着自己的页表，所以对于同一个虚拟地址空间，可以映射为不同的物理内存）
好处：
1、简化链接：因为虚拟地址的存在，使得不管代码和数据位于物理地址哪里，可以使用同一种规定好的格式（比如。规定代码段从A开始之后是B，处理这个格式映射到实际物理地址，这样就能方便处理。这样就容易链接代码和数据）
2、简化加载：比如要加载一个文件进入一个进程中，就先为这个文件创建虚拟页，然后把它标记为未缓存的，再用页表指向它，于是会发生使用牺牲页算法，使得加载加速，并且根据链接的准则，使得它们能放到合适的位置
3、简化共享：因为虚拟地址是规定好的，有格式的，所以更能实现不同的进程间的共享(因为不同的进程可以使用相同的物理地址，所以能够共享数据)******核心：不同的进程能映射到相同的物理地址
4、简化内存分配：因为虚拟地址的存在，所以不用在意数据实际放在物理地址的哪里，只需要放在合适的虚拟地址处，核心：任意位置的物理页面
9.5
虚拟内存作为内存的保护工具：实现比如：不能修改只读的文件，不能修改内核文件等等（权限）
提供独立的地址空间 使得区分不同进程的私有内存变得容易：因为每个进程有着自己独立的页表 所以很容易实现某些物理空间 只能自己映射到 从而实现了自己的私有内存
更进一步的想法：因为每次由虚拟地址变为物理地址需要用到页表里的PTE，所以扩展PTE的标识位，就能很简单的实现权限的控制   *****利用PTE的标识位 实现权限的控制****
9.6
地址翻译的细节：
利用虚拟地址的 虚拟页号 先寻找对应的页表有效位（PTE），然后把找到的页表中的页的物理页号++++++加+++虚拟页偏移量，就得到了对应的物理地址，实现了地址的翻译
*页面命中的时候：    先来一个虚拟地址，把它传送给MMU，MMU生成对应的PTE地址，拿到主存（高速缓存）中去找对应的那个，主存（高速缓存）向mmu返回带全部信息的PTE（PTE仅是标记位，它后面还有物理页号）
然后，在MMU中利用返回的PTE和虚拟地址，可以构造出对应的物理地址，并把它传说给主存（高速缓存），主存（高速缓存）利用物理地址把里面存储的数据对象传送给处理器。
*页面不命中的情况：     就是在页面命中的情况中，发现PTE里的标记位显示为未缓存，于是启动牺牲页算法，重新缓存需要的页面，然后再重新执行。
9.6.1
高速缓存（L1）使用物理寻址（更快），（我猜是因为高速缓存（L1）是位于结构体的最顶层，当访问到它的时候，已经非常的深入了，并且已经能非常的快速了，所以不再拐弯抹角的使用虚拟内存（反而更慢））
9.6.2
翻译后备缓冲器（TLB）：相当于就是一个缓存，缓存的对象是PTE，使得PTE的查找更加迅速
它的构造是有一定规则的，利用这个规则能更好的提取PTE
9.6.3
如果只是一个单独的页表来进行地址翻译，会让这个页表很大，于是需要压缩页表：使用层次结构的页表
层次结构：相当于先把一级页表分割成片（减小了需要的内存），再根据一级页表里每个片是否有被分配，来*****构造*****二级页表，相当于：从第一个被分配的片开始，只有到第二个被分配的片，才会映射去创造一个二级页表
这样一来：1、当一级页表里面有大量空的时候，会在二级页表中节省大量的空间 2、只有一级页表是一开始就需要存在在主存中的，二级页表是根据一级页表需要时再创建出来的（只有常用的二级页表才会一开始缓存再主存中）
于是就可以使用多级页表了，由于在页表中需要利用PTE的寻找，这时TLB（PTE的缓存）就起了很大的作用，使得多级页表快了起来
这样，速度即没有降低很多，还节省了大量的空间
9.6.4
一个地址翻译的例子
9.7 intel/linux的内存系统 案例
主要实现的就是上述的地址翻译和牺牲页算法































